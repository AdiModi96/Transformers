{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f1da21-10f4-4292-b267-f7a29afdb1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Module\n",
    "from torch import functional as F\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b802ee5-270c-44b0-a84d-496d2a4d059e",
   "metadata": {},
   "source": [
    "**Input & Model Dimensions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f316c711-1e04-45d8-bc97-a2e07dedd747",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "max_seq_length = 2**14\n",
    "seq_length = 100\n",
    "num_heads = 4\n",
    "d_model = 32\n",
    "d_q = d_k = d_v = d_model // num_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370f3d2b-3dbc-48c8-ae58-90d0aea5565c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(size=(batch_size, seq_length, d_model))\n",
    "print('Input vector size:', x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d4f9e4-ebb1-4d60-abe5-9091ab6dbf60",
   "metadata": {},
   "source": [
    "# Multi Head Attention Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26627ca9-a9e7-4696-95b5-3b8f7556ffeb",
   "metadata": {},
   "source": [
    "## Computing `Q`, `K` & `V` Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68012b1-ffea-4f25-aa01-bbcb2bf173d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_q = nn.Linear(in_features=d_model, out_features=d_model)\n",
    "W_k = nn.Linear(in_features=d_model, out_features=d_model)\n",
    "W_v = nn.Linear(in_features=d_model, out_features=d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd174e3-3abd-448d-857f-89eb99a19bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = W_q(x)\n",
    "print('Query vector size', Q.size())\n",
    "K = W_k(x)\n",
    "print('Key vector size', K.size())\n",
    "V = W_v(x)\n",
    "print('Value vector size', V.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc48f8d-0113-4abf-9248-636682883f12",
   "metadata": {},
   "source": [
    "## Splitting the `Q`, `K` & `V` vectors into various heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cc35be-ec16-464a-b9a0-eb19efa743cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = Q.view(batch_size, seq_length, num_heads, d_q)\n",
    "print('Query vector size', Q.size())\n",
    "\n",
    "K = K.view(batch_size, seq_length, num_heads, d_k)\n",
    "print('Key vector size', K.size())\n",
    "\n",
    "V = V.view(batch_size, seq_length, num_heads, d_v)\n",
    "print('Value vector size', V.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb10756d-f26e-43f3-8276-4d51872e0163",
   "metadata": {},
   "source": [
    "## Scaled Dot-Product Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd0b1b1-0d7a-48c5-b2de-9beff71fbf5d",
   "metadata": {},
   "source": [
    "### Permuting the tensors to compute the matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7450fcb3-2868-48c5-ad6d-44d6d60f283d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = Q.permute(0, 2, 1, 3)\n",
    "print('Query vector size', Q.size())\n",
    "\n",
    "K = K.permute(0, 2, 3, 1)\n",
    "print('Key vector size', K.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8dfff9-9f1f-4b20-9def-e442f5797960",
   "metadata": {},
   "source": [
    "### Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2f5ac7-b890-4494-b7c0-db99c3842373",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_scores = Q @ K\n",
    "print('Attention score vector size', attention_scores.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd6572b-cc21-4aa8-a783-13ad5a65599a",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692e7235-8e4c-429d-91d0-45a3eec1f7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_scores = attention_scores / math.sqrt(d_k)\n",
    "print('Attention score vector size', attention_scores.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e28802-1c63-4eb8-9fb9-ace3ffe39400",
   "metadata": {},
   "source": [
    "### Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6180458-0581-4f5a-91e2-643a7eeeaf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_mask = torch.ones(size=(seq_length, seq_length), dtype=torch.bool)\n",
    "print('Full mask size vector size', full_mask.size())\n",
    "\n",
    "lower_triangle_mask = torch.tril(torch.ones(size=(seq_length, seq_length), dtype=torch.bool))\n",
    "print('Lower triangle mask vector size', lower_triangle_mask.size())\n",
    "\n",
    "attention_scores = attention_scores.masked_fill(~full_mask, -1e15)\n",
    "print('Attention scores vector size', attention_scores.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f064023c-0ec0-43a8-a708-8fa363b579e2",
   "metadata": {},
   "source": [
    "### Softmax-ing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a743e551-1789-49ee-be61-208f6bae4e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_probabilities = attention_scores.softmax(dim=-1)\n",
    "print('Attention probabilities vector size', attention_probabilities.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c22346b-732b-4060-a9ad-093c36bd6b0e",
   "metadata": {},
   "source": [
    "### Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be54e7be-5153-40de-808e-2a7ffb1e9ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_dot_product_attention_output = (attention_probabilities @ V.permute(0, 2, 1, 3)).permute(0, 2, 1, 3)\n",
    "print('Scaled dot-product attention output vector size', scaled_dot_product_attention_output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260e0523-f029-49ca-b79c-c2a655247b6b",
   "metadata": {},
   "source": [
    "## Concat (Merging Heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d58d7c-3b98-4ac4-8e5a-7221eb15c7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_scaled_dot_product_attention_output = scaled_dot_product_attention_output.contiguous().view(batch_size, seq_length, d_model)\n",
    "print('Concatenated scaled dot-product attention output vector size', concatenated_scaled_dot_product_attention_output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25d059f-5fc5-4716-b18e-db7002e80f1a",
   "metadata": {},
   "source": [
    "## Linear Layer(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5bf10f-18f4-4cd7-99c3-4c289dacb0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_o = nn.Linear(in_features=d_model, out_features=d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07767760-1a35-4d05-956e-74565c46e569",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_head_attention_output = W_o(concatenated_scaled_dot_product_attention_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc0f370-b43c-471c-b042-ff8758f001c8",
   "metadata": {},
   "source": [
    "# Residual Connection & Layer Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418d16d3-13a3-46bb-bdfd-4afff4820b22",
   "metadata": {},
   "source": [
    "## Residual Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a057f6-f99a-4d39-9ee6-ea79eebd2c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x + multi_head_attention_output\n",
    "print('Residual connection output vector size', x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0f75dc-d17b-430f-83d3-0a0bfd1736be",
   "metadata": {},
   "source": [
    "## Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be7a824-827a-4270-8086-5ffe30e19cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer_after_multi_head_attention = nn.LayerNorm(d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b2b790-dd1d-4551-b65c-0757a63a4728",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = normalization_layer_after_multi_head_attention(x)\n",
    "print('Layer normalized vector size', x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63e9f2d-31ee-4caf-870b-706db3b50502",
   "metadata": {},
   "source": [
    "# Point-wise Feed-forward Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4680c374-0a32-4c64-a04e-db9a888727e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_forward = nn.Sequential(\n",
    "    nn.Linear(in_features=d_model, out_features=d_model),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(in_features=d_model, out_features=d_model)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2ea330-df2e-4d5f-8075-da9b25a9b8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_forward_output = feed_forward(x)\n",
    "print('Feed forward output vector size', feed_forward_output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737dbfc1-9ae3-4695-9e51-00b8b2eb199f",
   "metadata": {},
   "source": [
    "# Residual Connection & Layer Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb88340b-13b7-476a-b471-2a70433ba9f6",
   "metadata": {},
   "source": [
    "## Residual Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef45611-f1b6-46fd-ae64-b323b4e8398c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x + feed_forward_output\n",
    "print('Residual connection output vector size', x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192893a8-9f8b-43d7-bfe9-7643bd24cfc1",
   "metadata": {},
   "source": [
    "## Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b55615-1ea1-4005-9b21-b404cf546fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer_after_feed_forward = nn.LayerNorm(d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd3e669-1d07-43a1-940a-ea8ed3866b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = normalization_layer_after_feed_forward(x)\n",
    "print('Layer normalized vector size', x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff47adab-898a-48ca-a080-ea49fcd01d17",
   "metadata": {},
   "source": [
    "# Positional Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8e8023-4749-4cb6-b79f-6431d3099577",
   "metadata": {},
   "outputs": [],
   "source": [
    "positional_embeddings = torch.zeros(max_seq_length, d_model)\n",
    "n = 10000\n",
    "\n",
    "for pos in torch.arange(0, max_seq_length, dtype=torch.int):\n",
    "    i = torch.arange(0, d_model // 2)\n",
    "    positional_embeddings[pos, 0::2] = torch.sin(pos / n**(2 * i / d_model))\n",
    "    positional_embeddings[pos, 1::2] = torch.cos(pos / n**(2 * i / d_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819e23a2-5814-4e8a-8915-9d686940f9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(positional_embeddings[2].unsqueeze(0).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a73f09-c832-45e2-ace7-bdfe1c99de6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
