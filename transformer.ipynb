{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c883724-e340-4a80-bae2-8c86446d49d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Module\n",
    "from torch import functional as F\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555f2276-a0f0-44ff-abd5-c6c1d06256b0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<img src=\"./reference_images/architecture_diagrams/transformer.jpg\" alt=\"The Transformer Architecture\" style=\"width: 50%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a3e708-5abd-4812-a3a6-1791b51561f7",
   "metadata": {},
   "source": [
    "# Positional Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9979c88-cf85-4023-80e3-5731aef7d6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncodings(nn.Module):\n",
    "    def __init__(self, max_seq_length, d_model, n=10000):\n",
    "        super(PositionalEncodings, self).__init__()\n",
    "        \n",
    "        # Please, keep the value of d_model even\n",
    "        self.positional_encodings = torch.zeros(max_seq_length, d_model)\n",
    "\n",
    "        for pos in torch.arange(0, max_seq_length, dtype=torch.int):\n",
    "            i = torch.arange(0, d_model // 2)\n",
    "            self.positional_encodings[pos, 0::2] = torch.sin(pos / n**(2 * i / d_model))\n",
    "            self.positional_encodings[pos, 1::2] = torch.cos(pos / n**(2 * i / d_model))\n",
    "\n",
    "        self.register_buffer('pe', self.positional_encodings)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x + self.positional_encodings[:, seq_length]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1745591a-f7aa-4b12-bf05-8b77504d7f8e",
   "metadata": {},
   "source": [
    "# Transformer Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b983acd-6bee-4b2b-9b32-db79e3828b64",
   "metadata": {},
   "source": [
    "## Multi-head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bab566-bcd6-4ddb-bc33-cd7a3b3ee33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        assert d_model % num_heads == 0, 'Since d_model is split across attention heads, d_model should be divisible by num_heads'\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_q = self.d_k = self.d_v = d_model // num_heads\n",
    "\n",
    "        self.W_q = nn.Linear(in_features=self.d_model, out_features=self.d_model)\n",
    "        self.W_k = nn.Linear(in_features=self.d_model, out_features=self.d_model)\n",
    "        self.W_v = nn.Linear(in_features=self.d_model, out_features=self.d_model)\n",
    "        self.W_o = nn.Linear(in_features=self.d_model, out_features=self.d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        attention_scores = torch.matmul(Q.permute(0, 2, 1, 3), K.permute(0, 2, 3, 1)) / math.sqrt(self.d_k) # batch_size × num_heads × seq_length × seq_length\n",
    "\n",
    "        if mask is not None:\n",
    "            attention_scores = attention_scores.masked_fill(mask == 0, -1e15)\n",
    "\n",
    "        attention_probabilities = torch.softmax(attention_scores, dim=-1) # batch_size × num_heads × seq_length × seq_length\n",
    "        scaled_dot_product_attention_output = torch.matmul(attention_probabilities, V.permute(0, 2, 1, 3)) # batch_size × num_heads × seq_length × d_k\n",
    "        scaled_dot_product_attention_output = scaled_dot_product_attention_output.permute(0, 2, 1, 3) # batch_size × seq_length × num_heads × d_k\n",
    "        return scaled_dot_product_attention_output\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_length, d_model = x.size() \n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k)\n",
    "\n",
    "    def merge_heads(self, x):\n",
    "        batch_size, seq_length, num_heads, d_k = x.size()\n",
    "        return x.contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        Q = self.W_q(Q) # batch_size × seq_length × d_model\n",
    "        Q = self.split_heads(Q) # batch_size × seq_length × num_heads × d_k\n",
    "        \n",
    "        K = self.W_k(K) # batch_size × seq_length × d_model\n",
    "        K = self.split_heads(K) # batch_size × seq_length × num_heads × d_k\n",
    "        \n",
    "        V = self.W_v(V) # batch_size × seq_length × d_model\n",
    "        V = self.split_heads(V) # batch_size × seq_length × num_heads × d_k\n",
    "\n",
    "        scaled_dot_product_attention_output = self.scaled_dot_product_attention(Q, K, V, mask) # batch_size × seq_length × num_heads × d_k\n",
    "        concatenated_scaled_dot_product_attention_output = merge_heads(scaled_dot_product_attention_output) # batch_size × seq_length × d_model\n",
    "\n",
    "        multi_head_attention_output = self.W_o(concatenated_scaled_dot_product_attention_output) # batch_size × seq_length × d_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a304d4-6990-4cd9-9720-714850b945ff",
   "metadata": {},
   "source": [
    "## Point-wise Feed Forward Network\n",
    "This is a slightly extended version of feed-forward network mentioned in the original paper. Here, instead of having 1 hidden layer, one can customize it to have as many as they'd want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dc49ad-80ae-4d5f-94d5-f95b5073b649",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_hiddens=[]):\n",
    "        super(PointWiseFeedForward, self).__init__()\n",
    "\n",
    "        linear_layers = []\n",
    "        if len(d_hiddens) == 0:\n",
    "            self.linear_layers.append(nn.Linear(in_features=self.d_model, out_features=self.d_model))\n",
    "        else:\n",
    "            in_features = d_model\n",
    "            for d_hidden in d_hiddens:\n",
    "                self.linear_layers.append(nn.Linear(in_features=in_features, out_features=d_hidden))\n",
    "                self.linear_layers.append(nn.ReLU(inplace=True))\n",
    "                in_features = d_hidden\n",
    "            self.linear_layers.append(nn.Linear(in_features=in_features, out_features=d_model))\n",
    "\n",
    "        self.feed_fowrard = nn.Sequential(*linear_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.feed_fowrard(x) # batch_size × seq_length × d_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a5df26-0195-473c-81ed-b79add182964",
   "metadata": {},
   "source": [
    "## Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6234cfb1-2a3a-4292-843d-3c9abf4e5e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_hiddens, dropout_probability):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.multi_head_self_attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.layer_normalization_after_self_attention = nn.LayerNorm(d_model)\n",
    "        self.point_wise_feed_forward = PointWiseFeedForward(d_model, d_hiddens)\n",
    "        self.layer_normalization_after_feed_forward = nn.LayerNorm(d_model)\n",
    "        sefl.dropout = nn.Dropout(dropout_probability)\n",
    "    \n",
    "    def forward(self, x, self_attention_mask):\n",
    "        multi_head_self_attention_output = self.multi_head_self_attention(x, x, x, self_attention_mask)  # batch_size × seq_length × d_model\n",
    "        x = self.layer_normalization_after_self_attention(x + self.dropout(multi_head_self_attention_output))  # batch_size × seq_length × d_model\n",
    "        point_wise_feed_forward_output = self.point_wise_feed_forward(x)  # batch_size × seq_length × d_model\n",
    "        x = self.layer_normalization_after_feed_forward(x + self.dropout(point_wise_feed_forward_output))  # batch_size × seq_length × d_model\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00a9e64-c7e0-4649-beb1-1a49cd0d4efd",
   "metadata": {},
   "source": [
    "## Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5130eb25-4be1-406b-9bc6-e40eb2d3ebbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_hiddens, dropout_probability):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.multi_head_self_attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.layer_normalization_after_self_attention = nn.LayerNorm(d_model)\n",
    "        self.multi_head_cross_attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.layer_normalization_after_cross_attention = nn.LayerNorm(d_model)\n",
    "        self.point_wise_feed_forward = PointWiseFeedForward(d_model, d_hiddens)\n",
    "        self.layer_normalization_after_feed_forward = nn.LayerNorm(d_model)\n",
    "        sefl.dropout = nn.Dropout(dropout_probability)\n",
    "    \n",
    "    def forward(self, x, encoder_output, self_attention_mask, cross_attention_mask):\n",
    "        multi_head_self_attention_output = self.multi_head_self_attention(x, x, x, self_attention_mask)  # batch_size × seq_length × d_model\n",
    "        x = self.layer_normalization_after_self_attention(x + self.dropout(multi_head_self_attention_output))  # batch_size × seq_length × d_model\n",
    "        multi_head_cross_attention_output = self.multi_head_cross_attention(x, encoder_output, encoder_output, cross_attention_mask)  # batch_size × seq_length × d_model\n",
    "        x = self.layer_normalization_after_cross_attention(x + self.dropout(multi_head_cross_attention_output))  # batch_size × seq_length × d_model\n",
    "        point_wise_feed_forward_output = self.point_wise_feed_forward(x)  # batch_size × seq_length × d_model\n",
    "        x = self.layer_normalization_after_feed_forward(x + self.dropout(point_wise_feed_forward_output))  # batch_size × seq_length × d_model\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573a659f-1a9d-45ad-8746-4ee8dc15d9c8",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283be7be-a783-47ac-9684-e3df8acb59d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, max_seq_length, vocab_size, d_model, num_encoder_layers, num_decoder_layers, num_heads, d_hiddens, dropout_probability):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.embeddings = nn.Embeddings(vocab_size, d_model)\n",
    "        self.positional_encodings = PositionalEmbeddings(max_seq_length=max_seq_length, d_model)\n",
    "        \n",
    "        encoder_layers = [EncoderLayer(d_model, num_heads, d_hiddens, dropout_probability) for encoder_block_idx in range(num_encoder_layers)]\n",
    "        self.encoder = nn.ModuleList(*encoder_layers)\n",
    "\n",
    "        decoder_layers = [DecoderLayer(d_model, num_heads, d_hiddens, dropout_probability) for encoder_block_idx in range(num_decoder_layers)]\n",
    "        self.decoder = nn.ModuleList(*decoder_layers)\n",
    "\n",
    "    def forward(self, inputs, outputs):\n",
    "        inputs # batch_size × seq_length\n",
    "        outputs # batch_size × seq_length\n",
    "        \n",
    "        embedded_inputs = self.embeddings(inputs) # batch_size × seq_length * d_model \n",
    "        embedded_outputs = self.embeddings(outputs) # batch_size × seq_length * d_model\n",
    "        \n",
    "        position_encoded_inputs = self.positional_encodings(embedded_inputs) # batch_size × seq_length * d_model\n",
    "        position_encoded_outputs = self.positional_encodings(embedded_outputs) # batch_size × seq_length * d_model\n",
    "\n",
    "        encoder_outputs = position_encoded_inputs # batch_size × seq_length * d_model\n",
    "        for encoder_layer in self.encoder:\n",
    "            encoder_outputs = encoder_layer(position_encoded_inputs, encoder_self_attention_mask) # batch_size × seq_length * d_model\n",
    "\n",
    "        decoder_outputs = position_encoded_outputs # batch_size × seq_length * d_model\n",
    "        for decoder_layer in self.decoder:\n",
    "            decoder_outputs = decoder_layer(position_encoded_outputs, encoder_outputs, decoder_self_attention_mask, decoder_cross_attention_mask) # batch_size × seq_length * d_model\n",
    "\n",
    "        return decoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5673e574-4b13-4753-a351-fa96ba6b5598",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
