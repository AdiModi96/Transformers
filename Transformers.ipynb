{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c883724-e340-4a80-bae2-8c86446d49d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Module\n",
    "from torch import functional as F\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555f2276-a0f0-44ff-abd5-c6c1d06256b0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<img src=\"./reference_images/architecture_diagrams/transformer.jpg\" alt=\"The Transformer Architecture\" style=\"width: 50%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1745591a-f7aa-4b12-bf05-8b77504d7f8e",
   "metadata": {},
   "source": [
    "# Transformer Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b983acd-6bee-4b2b-9b32-db79e3828b64",
   "metadata": {},
   "source": [
    "## Multi-head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bab566-bcd6-4ddb-bc33-cd7a3b3ee33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        assert d_model % num_heads == 0, 'Since d_model is split across attention heads, d_model should be divisible by num_heads'\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_q = self.d_k = self.d_v = d_model // num_heads\n",
    "\n",
    "        self.W_q = nn.Linear(in_features=self.d_model, out_features=self.d_model)\n",
    "        self.W_k = nn.Linear(in_features=self.d_model, out_features=self.d_model)\n",
    "        self.W_v = nn.Linear(in_features=self.d_model, out_features=self.d_model)\n",
    "        self.W_o = nn.Linear(in_features=self.d_model, out_features=self.d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        attention_scores = torch.matmul(Q, K) / math.sqrt(self.d_k)\n",
    "\n",
    "        attention_probabilities = torch.softmax(attention_scores)\n",
    "        output = torch.matmul(attention_probabilities, V)\n",
    "        return output\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k)\n",
    "\n",
    "    def merge_heads(self, x):\n",
    "        pass\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        Q = self.split_heads(Q)\n",
    "        K = self.split_heads(K)\n",
    "        V = self.split_heads(V)\n",
    "\n",
    "        attention_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "\n",
    "        output = self.W_o(self.merge_heads(attention_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a304d4-6990-4cd9-9720-714850b945ff",
   "metadata": {},
   "source": [
    "## Point-wise Feed Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dc49ad-80ae-4d5f-94d5-f95b5073b649",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_hiddens=[]):\n",
    "        super(PointWiseFeedForward, self).__init__()\n",
    "\n",
    "        linear_layers = []\n",
    "        if len(d_hiddens) == 0:\n",
    "            self.linear_layers.append(nn.Linear(in_features=self.d_model, out_features=self.d_model))\n",
    "        else:\n",
    "            in_features = d_model\n",
    "            for d_hidden in d_hiddens:\n",
    "                self.linear_layers.append(nn.Linear(in_features=in_features, out_features=d_hidden))\n",
    "                self.linear_layers.append(nn.ReLU(inplace=True))\n",
    "                in_features = d_hidden\n",
    "            self.linear_layers.append(nn.Linear(in_features=in_features, out_features=d_model))\n",
    "\n",
    "        self.feed_fowrard = nn.Sequential(*linear_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.feed_fowrard(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a5df26-0195-473c-81ed-b79add182964",
   "metadata": {},
   "source": [
    "## Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6234cfb1-2a3a-4292-843d-3c9abf4e5e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_hiddens, dropout_probability):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.multi_head_attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.point_wise_feed_forward = PointWiseFeedForward(d_model, d_hiddens)\n",
    "        self.layer_normalization_after_self_attention = nn.LayerNorm(d_model)\n",
    "        self.layer_normalization_after_feed_forward = nn.LayerNorm(d_model)\n",
    "        sefl.dropout = nn.Dropout(dropout_probability)\n",
    "    \n",
    "    def forward(x, mask):\n",
    "        multi_head_attention_output = self.multi_head_attention(x, x, x, mask)\n",
    "        x = self.layer_normalization_after_self_attention(x + self.dropout(multi_head_attention_output))\n",
    "        point_wise_feed_forward_output = self.point_wise_feed_forward(x)\n",
    "        x = self.layer_normalization_after_feed_forward(x + self.dropout(point_wise_feed_forward_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f678d41c-936b-47df-b299-a57d3efad96b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
