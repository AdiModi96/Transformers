{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c883724-e340-4a80-bae2-8c86446d49d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555f2276-a0f0-44ff-abd5-c6c1d06256b0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<img src=\"./reference_images/architecture_diagrams/transformer.jpg\" alt=\"The Transformer Architecture\" style=\"width:50%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a3e708-5abd-4812-a3a6-1791b51561f7",
   "metadata": {},
   "source": [
    "# Positional Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9979c88-cf85-4023-80e3-5731aef7d6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length, n=10000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        assert d_model % 2 == 0, 'Due to implementation limitations, please keep the value of d_model even'\n",
    "        self.positional_encodings = torch.zeros(max_seq_length, d_model)  # max_seq_length × d_model\n",
    "\n",
    "        for pos in torch.arange(0, max_seq_length, dtype=torch.int):\n",
    "            i = torch.arange(0, d_model // 2)\n",
    "            self.positional_encodings[pos, 0::2] = torch.sin(pos / n ** (2 * i / d_model))\n",
    "            self.positional_encodings[pos, 1::2] = torch.cos(pos / n ** (2 * i / d_model))\n",
    "\n",
    "        self.register_buffer('pe', self.positional_encodings)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input(s)\n",
    "        x  # batch_size × seq_length × d_model\n",
    "\n",
    "        # Operation(s)\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        positional_encoding_output = x + self.positional_encodings[seq_length, :]  # batch_size × seq_length × d_model\n",
    "\n",
    "        # Output(s)\n",
    "        positional_encoding_output  # batch_size × seq_length × d_model\n",
    "        return positional_encoding_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b983acd-6bee-4b2b-9b32-db79e3828b64",
   "metadata": {},
   "source": [
    "# Multi-head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b088942-4b7e-4632-b3da-b04b3a72a889",
   "metadata": {},
   "source": [
    "<figure style=\"display:flex\">\n",
    "    <img src=\"./reference_images/architecture_diagrams/scaled_dot_product_attention.jpg\" alt=\"Scaled Dot-Product Attention\" style=\"width:20%;height:20%\"/>\n",
    "    <img src=\"./reference_images/architecture_diagrams/multi-head_attention.jpg\" alt=\"Multi-head Attention\" style=\"width: 35%;height:35%\" margin=\"10%\"/>\n",
    "</figure>\n",
    "Scaled Dot-Product Attention (left) & Multi-head Attention (right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bab566-bcd6-4ddb-bc33-cd7a3b3ee33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        assert d_model % num_heads == 0, 'Since d_model is split across attention heads, d_model should be divisible by num_heads'\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_q = self.d_k = self.d_v = d_model // num_heads\n",
    "\n",
    "        self.W_q = nn.Linear(in_features=self.d_model, out_features=self.d_model)\n",
    "        self.W_k = nn.Linear(in_features=self.d_model, out_features=self.d_model)\n",
    "        self.W_v = nn.Linear(in_features=self.d_model, out_features=self.d_model)\n",
    "        self.W_o = nn.Linear(in_features=self.d_model, out_features=self.d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        # Input(s)\n",
    "        Q  # batch_size × seq_length × num_heads × d_k\n",
    "        K  # batch_size × seq_length × num_heads × d_k\n",
    "        V  # batch_size × seq_length × num_heads × d_k\n",
    "        mask  # seq_length × seq_length\n",
    "\n",
    "        # Operation(s)\n",
    "        Q = Q.permute(0, 2, 1, 3)  # batch_size × num_heads × seq_length × d_k\n",
    "        K = K.permute(0, 2, 3, 1)  # batch_size × num_heads × d_k × seq_length\n",
    "\n",
    "        attention_scores = torch.matmul(Q, K) / (self.d_k ** 0.5)  # seq_length × seq_length\n",
    "        if mask is not None:\n",
    "            attention_scores = attention_scores.masked_fill(~mask, value=-1e15)  # seq_length × seq_length\n",
    "        attention_probabilities = torch.softmax(attention_scores, dim=-1)  # seq_length × seq_length\n",
    "\n",
    "        V = V.permute(0, 2, 1, 3)  # batch_size × num_heads × seq_length × d_k\n",
    "        scaled_dot_product_attention_output = torch.matmul(\n",
    "            attention_probabilities, V)  # batch_size × num_heads × seq_length × d_k\n",
    "\n",
    "        scaled_dot_product_attention_output = scaled_dot_product_attention_output.permute(\n",
    "            0, 2, 1, 3)  # batch_size × seq_length × num_heads × d_k\n",
    "\n",
    "        # Output(s)\n",
    "        scaled_dot_product_attention_output  # batch_size × seq_length × num_heads × d_k\n",
    "        return scaled_dot_product_attention_output\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        # Input(s)\n",
    "        x  # batch_size × seq_length × d_model\n",
    "\n",
    "        # Operation(s)\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        x = x.view(batch_size, seq_length, self.num_heads, self.d_k)  # batch_size × seq_length × num_heads × d_k\n",
    "\n",
    "        # Output(s)\n",
    "        x  # batch_size × seq_length × num_heads × d_k\n",
    "        return x\n",
    "\n",
    "    def merge_heads(self, x):\n",
    "        # Input(s)\n",
    "        x  # batch_size × seq_length × num_heads × d_k\n",
    "\n",
    "        # Operation(s)\n",
    "        batch_size, seq_length, num_heads, d_k = x.size()\n",
    "        x = x.contiguous().view(batch_size, seq_length, self.d_model)  # batch_size × seq_length × d_model\n",
    "\n",
    "        # Output(s)\n",
    "        x  # batch_size × seq_length × d_model\n",
    "        return x\n",
    "\n",
    "    def forward(self, for_Q, for_K, for_V, mask=None):\n",
    "        # Input(s)\n",
    "        for_Q  # batch_size × seq_length × input_size\n",
    "        for_K  # batch_size × seq_length × input_size\n",
    "        for_V  # batch_size × seq_length × input_size\n",
    "        mask  # seq_length × seq_length\n",
    "\n",
    "        # Operation(s)\n",
    "        Q = self.W_q(for_Q)  # batch_size × seq_length × d_model\n",
    "        Q = self.split_heads(Q)  # batch_size × seq_length × num_heads × d_k\n",
    "\n",
    "        K = self.W_k(for_K)  # batch_size × seq_length × d_model\n",
    "        K = self.split_heads(K)  # batch_size × seq_length × num_heads × d_k\n",
    "\n",
    "        V = self.W_v(for_V)  # batch_size × seq_length × d_model\n",
    "        V = self.split_heads(V)  # batch_size × seq_length × num_heads × d_k\n",
    "\n",
    "        scaled_dot_product_attention_output = self.scaled_dot_product_attention(\n",
    "            Q, K, V, mask)  # batch_size × seq_length × num_heads × d_k\n",
    "        concatenated_scaled_dot_product_attention_output = self.merge_heads(\n",
    "            scaled_dot_product_attention_output)  # batch_size × seq_length × d_model\n",
    "\n",
    "        multi_head_attention_output = self.W_o(\n",
    "            concatenated_scaled_dot_product_attention_output)  # batch_size × seq_length × d_model\n",
    "\n",
    "        # Output(s)\n",
    "        multi_head_attention_output  # batch_size × seq_length × d_model\n",
    "        return multi_head_attention_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a304d4-6990-4cd9-9720-714850b945ff",
   "metadata": {},
   "source": [
    "# Point-wise Feed Forward Network\n",
    "This is a slightly extended version of feed-forward network mentioned in the original paper. Here, instead of having 1 hidden layer, one can customize it to have as many as they'd want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dc49ad-80ae-4d5f-94d5-f95b5073b649",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_hiddens=[]):\n",
    "        super(PointWiseFeedForward, self).__init__()\n",
    "\n",
    "        linear_layers = []\n",
    "        if len(d_hiddens) == 0:\n",
    "            self.linear_layers.append(nn.Linear(in_features=self.d_model, out_features=self.d_model))\n",
    "        else:\n",
    "            in_features = d_model\n",
    "            for d_hidden in d_hiddens:\n",
    "                linear_layers.append(nn.Linear(in_features=in_features, out_features=d_hidden))\n",
    "                linear_layers.append(nn.ReLU(inplace=True))\n",
    "                in_features = d_hidden\n",
    "            linear_layers.append(nn.Linear(in_features=in_features, out_features=d_model))\n",
    "\n",
    "        self.feed_forward = nn.Sequential(*linear_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #  Input(s)\n",
    "        x  # batch_size × seq_length × d_model\n",
    "\n",
    "        # Operation(s)\n",
    "        x = self.feed_forward(x)  # batch_size × seq_length × d_model\n",
    "\n",
    "        # Output(s)\n",
    "        x  # batch_size × seq_length × d_model\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a5df26-0195-473c-81ed-b79add182964",
   "metadata": {},
   "source": [
    "# Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6234cfb1-2a3a-4292-843d-3c9abf4e5e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_hiddens, dropout_probability):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.multi_head_self_attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.layer_normalization_after_self_attention = nn.LayerNorm(d_model)\n",
    "        self.point_wise_feed_forward = PointWiseFeedForward(d_model, d_hiddens)\n",
    "        self.layer_normalization_after_feed_forward = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout_probability)\n",
    "\n",
    "    def forward(self, x, self_attention_mask):\n",
    "        # Input(s)\n",
    "        x  # batch_size × seq_length × d_model\n",
    "        self_attention_mask  # seq_length × seq_length\n",
    "\n",
    "        # Operation(s)\n",
    "        multi_head_self_attention_output = self.multi_head_self_attention(\n",
    "            for_Q=x, for_K=x, for_V=x,\n",
    "            mask=self_attention_mask)  # batch_size × seq_length × d_model\n",
    "        x = self.layer_normalization_after_self_attention(\n",
    "            x + self.dropout(multi_head_self_attention_output))  # batch_size × seq_length × d_model\n",
    "        point_wise_feed_forward_output = self.point_wise_feed_forward(x)  # batch_size × seq_length × d_model\n",
    "        x = self.layer_normalization_after_feed_forward(\n",
    "            x + self.dropout(point_wise_feed_forward_output))  # batch_size × seq_length × d_model\n",
    "\n",
    "        # Output(s)\n",
    "        x  # batch_size × seq_length × d_model\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00a9e64-c7e0-4649-beb1-1a49cd0d4efd",
   "metadata": {},
   "source": [
    "# Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5130eb25-4be1-406b-9bc6-e40eb2d3ebbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_hiddens, dropout_probability):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.multi_head_self_attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.layer_normalization_after_self_attention = nn.LayerNorm(d_model)\n",
    "        self.multi_head_cross_attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.layer_normalization_after_cross_attention = nn.LayerNorm(d_model)\n",
    "        self.point_wise_feed_forward = PointWiseFeedForward(d_model, d_hiddens)\n",
    "        self.layer_normalization_after_feed_forward = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout_probability)\n",
    "\n",
    "    def forward(self, x, encoder_output, self_attention_mask, cross_attention_mask):\n",
    "        # Input(s)\n",
    "        x  # batch_size × seq_length × d_model\n",
    "        self_attention_mask  # seq_length × seq_length\n",
    "        cross_attention_mask  # seq_length × seq_length\n",
    "\n",
    "        # Operation(s)\n",
    "        multi_head_self_attention_output = self.multi_head_self_attention(\n",
    "            for_Q=x, for_K=x, for_V=x,\n",
    "            mask=self_attention_mask)  # batch_size × seq_length × d_model\n",
    "        x = self.layer_normalization_after_self_attention(\n",
    "            x + self.dropout(multi_head_self_attention_output))  # batch_size × seq_length × d_model\n",
    "        multi_head_cross_attention_output = self.multi_head_cross_attention(\n",
    "            for_Q=x, for_K=encoder_output, for_V=encoder_output,\n",
    "            mask=cross_attention_mask)  # batch_size × seq_length × d_model\n",
    "        x = self.layer_normalization_after_cross_attention(\n",
    "            x + self.dropout(multi_head_cross_attention_output))  # batch_size × seq_length × d_model\n",
    "        point_wise_feed_forward_output = self.point_wise_feed_forward(x)  # batch_size × seq_length × d_model\n",
    "        x = self.layer_normalization_after_feed_forward(\n",
    "            x + self.dropout(point_wise_feed_forward_output))  # batch_size × seq_length × d_model\n",
    "\n",
    "        # Output(s)\n",
    "        x  # batch_size × seq_length × d_model\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
